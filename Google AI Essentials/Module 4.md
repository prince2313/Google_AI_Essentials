# Use AI Responsibly
## Understand bias in AI

- **Systemic bias:**
	- A tendency upheld by institutions that favors or disadvantage certain outcomes or groups.

- **Data bias:**
	- A circumstances in which systemic errors or prejudices lead to unfair or inaccurate information, resulting in biased outputs.

---
## Security and privacy risks of AI

- **Privacy:**
	- The right for a user to have control over how their personal information and data are collected, stored, and used.

- **Security:**
	- The act of safeguard personal information and private data, and ensuring that the system is secure by preventing unauthorized access.

- **Measure to protect privacy and security:**
	- Be aware of the tool's terms of use or service, privacy policy, and any associated risks.
	- Don't input personal or confidential information.
	- Stay up-to-date on the latest tools.

---
## Bias, Drift, and Knowledge Cutoff in AI: Ethical Considerations
### Harms and Biases
#### Data Biases
- **Definition:** Systemic errors or prejudices in data leading to biased outputs.
- **Types of Harm:**
  - **Allocative Harm:** AI systems deny opportunities, resources, or information based on biased outputs.
    - **Example:** AI tool for background checks incorrectly labels an applicant as high-risk due to a low credit score, leading to denial of housing.
  - **Quality-of-Service Harm:** AI tools perform inadequately for specific groups based on identity.
    - **Example:** Early speech-recognition technology struggled to recognize speech patterns of people with disabilities due to lack of diverse training data.
  - **Representational Harm:** AI systems reinforce stereotypes or biases based on social identities.
    - **Example:** Early translation tools skewed gender translations inaccurately, reinforcing gender stereotypes.
  - **Social System Harm:** AI tools amplify existing social inequalities or cause physical harm.
    - **Example:** Deepfakes can cause reputational damage or physical harm by creating realistic but fake images or videos of people.
  - **Interpersonal Harm:** AI is used to disadvantage individuals, affecting their relationships or sense of self.
    - **Example:** Manipulating in-home devices to harass someone can cause psychological harm and a loss of agency.

### Drift and Knowledge Cutoff
#### Drift
- **Definition:** Decline in model accuracy over time due to changes not reflected in training data.
- **Example:** A fashion trend prediction model from 2015 becomes inaccurate over time as consumer preferences evolve.

#### Knowledge Cutoff
- **Definition:** AI models trained up to a specific date lack knowledge of subsequent events.
- **Example:** A model trained in 2022 cannot provide information on technologies or events that occurred after 2022.

### Responsible AI Usage

#### Awareness
- Understanding these concepts helps in anticipating and mitigating the risks associated with AI.

#### Mitigation
- Regularly update models to reflect current data and trends to reduce drift and address knowledge cutoffs.

#### Ethical Practices
- Incorporate diverse and representative data to minimize biases and their harmful impacts.

### Further Exploration

- **Exercise:** [What Have Language Models Learned?](https://pair.withgoogle.com/explorables/fill-in-the-blank/) from Google PAIR Explorables. Interact with BERT to understand how data correlations can lead to biased outcomes.
- **Other Resources:** [PAIR AI Explorables](https://pair.withgoogle.com/explorables/) for more insights into responsible AI practices.

---
## Checklist for Using AI Responsibly
### Review AI Outputs

- Identify potential harms by prompting the AI tool with different examples and checking the outputs.
- Test the tool on topics you’re familiar with, so you can verify outputs with your own knowledge.
- To minimize the effects of hallucinations:
    - Always make sure your prompt provides context, includes an example, and states a request.
    - Avoid using a false premise in your input. Make sure your prompt is clear, specific, and accurate.

### Disclose Your Use of AI

- Tell your audience and anyone it might affect that you’ve used or are using AI. This step is particularly important in high-impact professional settings.
- Explain what type of tool you used, describe your intention, provide an overview of your use of AI, and offer any other information that could help your audience evaluate potential risks.
- Don’t copy and paste outputs generated by AI and pass them off as your own.

### Evaluate All Content Before You Share It

- Fact check content accuracy using search engines.
- Ask yourself: If this content turns out to be inaccurate or untrue, am I willing or able to correct my mistake? If not, reconsider sharing it.
- Remember the steps to SHARE, the [World Health Organization](https://www.who.int/news/item/22-09-2021-be-careful-what-you-share.-things-aren-t-always-what-they-seem-online/)’s mnemonic for thoughtful information sharing:
    - **Source:** Ensure content comes from credible and official sources.
    - **Headlines:** Read full articles, as headlines don't always tell the full story.
    - **Analyze:** Verify the facts to ensure they are true.
    - **Retouched:** Be cautious of misleading imagery in photos and videos.
    - **Errors:** Look out for typos and errors, which can indicate false information.

### Consider the Privacy and Security Implications of Using AI

- Only input essential information. Avoid sharing unnecessary, confidential, or private information to protect the security of people or organizations.
- Read supporting documents associated with the tools you’re using. Documentation about the model's training and privacy safeguards (e.g., terms and conditions) can be helpful resources.

### Consider the Effects of Using AI

- Ask yourself:
    - If I use AI for this particular task, will it hurt anyone around me?
    - Does it reinforce or uphold biases that may cause damage to any groups of people?

### Additional Resources

- [Download a copy of this checklist for using AI responsibly](https://docs.google.com/document/d/1_gZlh2oONOd1116f0EmYXHaVwxzcVbw8SZRm4y2aIF8/template/preview)

---